---
title: "Module 3 Assesment"
author: "BTRY 6020"
date: ""
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Question 1 (2 pts)
Suppose I believe my data is generated by the following model:
\[Y_i = b_0 + b_1 X_{i,1} + b_2X_{i,2} + b_3 X_{i,3} + b_4 X_{i,4} + \varepsilon_i.\]
I want to test the null hypothesis that $X_{i,2}$ is not associated with $Y_i$ after adjusting for $X_{i,1}$, $X_{i,3}$, and $X_{i,4}$. The alternative hypothesis is that there is some association between $X_{i,2}$ and $Y_i$, even after adjusting for the other covariates. What is the null hypothesis and the alternative hypothesis:


## Answer

\textcolor{red}{
$H_0:\;\;b_2=0$ \newline
$H_A:\;\;b_2\neq0$
}


# Question 2 (2 pts)
Suppose I gather $35$ observations and fit the model specified above. Given the output below, calculate the t-statistic for testing the hypothesis. Round this answer to two digits after the decimal.
\begin{table}[ht]
\centering
\begin{tabular}{rllll}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & 1.971 & 0.171 & 11.533 & 0 \\ 
  X1 & 0.861 & 0.212 & 4.063 & 0 \\ 
  X2 & 0.373 & 0.194 & ??? & ??? \\ 
  X3 & 1.078 & 0.188 & 5.738 & 0 \\ 
  X4 & -0.057 & 0.221 & -0.259 & 0.798 \\ 
   \hline
\end{tabular}
\end{table}


## Answer

\textcolor{red}{
$t=\frac{\hat{b}_2}{\hat{SE}\left(\hat{b}_2\right)}=\frac{0.373}{0.194}=1.923$\newline
}

# Question 3 (2 pts)
Suppose you are interested in testing the null hypothesis $H_0: b_2 = .5$. Given the table above, calculate the t-statistic for testing this null hypothesis. Round this answer to two digits after the decimal.

## Answer
\textcolor{red}{
$t=\frac{\hat{b}_2-b_2^{(0)}}{\hat{SE}\left(\hat{b}_2\right)}=\frac{0.373-0.5}{0.194}=-0.655$\newline
}

# Question 4 (2 pts)
You can use the \texttt{qt} function to get the cut-off from a T distribution. Specifically, the code below gets the cutoff so that the area to right of that cut-off is alpha / 2 for a T distribution with $z$ degrees of freedom. Calculate the p-value for the table in Question 2, and also for the null hypothesis in Question 3.
```{r, eval = F}
qt(alpha / 2, df = z, lower = F)
```

\textcolor{red}{
Problem 2: $p=P(|t_{n-p-1}|\geq1.923)=2\cdot P(t_{n-p-1}<-1.923)=2\times \texttt{pt(-1.923,35-4-1)}=0.064$\\
Problem 3: $p=P(|t_{n-p-1}|\geq 0.655)=2\cdot P(t_{n-p-1}<-0.655)=2\times \texttt{pt(-0.655,35-4-1)}=0.517$ 
}

# Question 5 (1 pt)
Calculate a 90\% confidence interval for the coefficient of $X_1$.


## Answer

\textcolor{red}{
$\hat{b}_1 \pm \hat{SE}\left(\hat{b}_1\right)\cdot t_{n-p-1}^{(\alpha/2)}$\\
$0.861 \pm (0.212)t_{35-4-1}^{(0.05)}$\\
$0.861 \pm (0.212)(1.697)$\\
$[0.501,1.221]$
}


# Question 6 (2 pts)
Consider two worlds. In both, you are interested in testing the null hypothesis that $H_0: b_1 = 0$ vs $H_A: b_1 \neq 0$. In the first setting $b_1 = 1$ and in the second setting $b_1 = 2$. If all other things are equal, in which setting do you have more power to reject the null hypothesis. Give a brief explanation of why?

## Answer

\textcolor{red}{
In both worlds, presuming all model assumptions are met, the estimates $\hat{b}_1$ are distributed normally around their true value. Since 2 is farther away from 0 than 1 is, the sampling distribution of  $\frac{\hat{b}_1}{\hat{\text{var}}(\hat b_1)}$will have more probability mass in the rejection region when $b_1=2$ when compared to the sampling distribution when $b_1=1$, assuming the variance of the residuals is the same in both settings. 
}

# Question 7 (2 pts)
Suppose you are interested in testing the null hypothesis that $H_0: b_1 = 0$ vs $H_A: b_1 \neq 0$. However, the true $b_1 = 1$. Suppose you are deciding to test the null hypothesis with either a $\alpha = .05$ or $\alpha = .1$ level test. All other things are equal, in which test would have more power to reject the null hypothesis. Give a brief explanation of why?

## Answer

\textcolor{red}{
With a larger $\alpha$, we are tolerating a higher false-positive (type 1) error rate. Thus, our rejection region is larger and we would reject the null hypothesis for smaller (in absolute value) t-statistics. This means we will have higher power to reject the null hypothesis when it is false. 
}


# Housing Data
Recall the housing data that we've been considering in lecture. We can load the data using the following code:

```{r}
fileName <- url("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/estate.csv")
housing_data <- read.csv(fileName)

head(housing_data)
```

There are 522 observations with the following variables:

* price: in 2002 dollars
* area: Square footage
* bed: number of bedrooms
* bath: number of bathrooms
* ac: central AC (yes/no)
* garage: number of garage spaces
* pool: yes/no
* year: year of construction
* quality: high/medium/low
* home style: coded 1 through 7
* lot size: sq ft 
* highway: near a highway (yes/no)

There is no age data in the table, but we can compute it on our own from the year variable
```{r}
housing_data$age <- 2002 - housing_data$year
```

# Question 8 (3 pts)
Let $\log(price)$ be the dependent variable. Suppose we are interested in the association of $\log(price)$ with the lot size, after conditioning for the area, age, and number of bedrooms. Estimate the linear coefficient of interest and give an interpretation of the point estimate. Form a 95\% confidence interval for the coefficient of interest.

## Answer
```{r}
lmod8 <- lm(log(price)~lot+area+age+bed,data=housing_data)
coef(lmod8)
confint(lmod8)
```
\textcolor{red}{
The point estimate for the coefficient of interest is $6.086 \cdot 10^{-6}$. This means that when comparing two houses which differ in lot size by 1 square foot, the price will go up by approximately 0.0006086 percent, holding area, age, and number of bedrooms constant. (Note that $e^{\hat{b}_1}-1$ is very close to $\hat{b}_1$ because $e^x-1 \approx x$ near $x=0$)
%
We are 95\% confident that $b_1$ lies between $2.84 \cdot 10^{-6}$ and $9.33 \cdot  10^{-6}$.
}

# Question 9 (3 pts)
Let $\log(price)$ be the dependent variable. Suppose we are interested in the association of $\log(price)$ with the number of bedrooms, after conditioning for the $\log(area)$, $\log(lot)$, and age. Conduct a hypothesis test with level $\alpha = .05$ for the null hypothesis that bedrooms is not associated with $\log(price)$ after conditioning for $\log(area)$, $\log(lot)$, and age. What is the resulting $t$ statistic? What is the result of the hypothesis test?

## Answer
```{r}
lmod9 <- lm(log(price)~bed+log(area)+log(lot)+age,data=housing_data)
summary(lmod9)
```

\textcolor{red}{
The t-statistic for this hypothesis test is $t=-0.547$. The resulting p-value is $0.585$ which indicates a lack of evidence that the number of beds is associated with the log price. We thus fail to reject the null hypothesis that $b_{bed}=0$.
}

# Question 10 (3 pts)
Let $\log(price)$ be the dependent variable. Suppose we are interested in the association of $\log(price)$ with quality of the house, after conditioning for the $\log(area)$, age, and number of bedrooms. Conduct a hypothesis test with level $\alpha = .05$ for the null hypothesis that quality is not associated with $\log(price)$ after conditioning for the area, age, and number of bedrooms. What is the resulting statistic? What is the result of the hypothesis test?

## Answer

\textcolor{red}{Since `quality` is a categorical variable, we must test the coefficients of all the dummy variables at the same time. For this, we conduct a regression F-test:}

```{r}
lmod10a <- lm(log(price)~quality + log(area) + age + bed, data=housing_data)
lmod10b <- lm(log(price)~          log(area) + age + bed, data=housing_data)
anova(lmod10a,lmod10b)
```
\textcolor{red}{
With an F statistic of $65.377$ on 516 and 518 degress of freedom, we find a p-value smaller than $2\cdot10^{-16}$. Thus we reject the null hypothesis and conclude that the quality of the house is associated with the price of the house. 
}




